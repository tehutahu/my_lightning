{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 4\n",
    "num_heads = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in_proj_weight',\n",
      "  Parameter containing:\n",
      "tensor([[ 0.0492,  0.1454, -0.3237, -0.5530],\n",
      "        [-0.4122, -0.4595, -0.1724, -0.2761],\n",
      "        [-0.4626,  0.2290, -0.5929, -0.3930],\n",
      "        [-0.2985, -0.1988,  0.4165,  0.6081],\n",
      "        [-0.1084, -0.1049,  0.2425, -0.0454],\n",
      "        [-0.1500,  0.5269,  0.5555, -0.1842],\n",
      "        [-0.3304,  0.6096, -0.3090, -0.0992],\n",
      "        [ 0.0750, -0.1702,  0.3382, -0.3463],\n",
      "        [ 0.0877, -0.2240, -0.4005, -0.3957],\n",
      "        [ 0.4808, -0.0672, -0.2906, -0.3212],\n",
      "        [-0.0817, -0.0162, -0.5907, -0.4768],\n",
      "        [ 0.5461,  0.0227,  0.2328, -0.5729]], requires_grad=True)),\n",
      " ('out_proj.weight',\n",
      "  Parameter containing:\n",
      "tensor([[ 0.4502, -0.4243,  0.1935, -0.2146],\n",
      "        [ 0.0306, -0.1774,  0.3395,  0.2386],\n",
      "        [-0.2647,  0.4024,  0.0799, -0.2737],\n",
      "        [-0.3745, -0.3256, -0.2285, -0.2394]], requires_grad=True)),\n",
      " ('out_proj.bias',\n",
      "  Parameter containing:\n",
      "tensor([-0.3763,  0.1134,  0.3692,  0.3514], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "model = nn.MultiheadAttention(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    bias=False,\n",
    ")\n",
    "pprint(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 16\n",
    "L = 10\n",
    "D = embed_dim\n",
    "X = torch.randn(B, L, D)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.permute(1, 0, 2) # Batch second (L, B, D)\n",
    "Q = K = V = X\n",
    "attn_output, attn_weights = model(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ MultiHeadAttention(Q,K,V)=Concat(head_1, ..., head_h)W^O \\\\\n",
    "head_i = Attention(QW^Q_i, KW^K_i, VW^V_i) \\\\\n",
    "Attention(Q,K,V) = softmax({QK^T \\over \\sqrt{d_Q}})V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = {name: param.data for name, param in model.named_parameters()}\n",
    "Wi = model_weights['in_proj_weight']\n",
    "Wo = model_weights['out_proj.weight']\n",
    "bias = model_weights['out_proj.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wi_q, Wi_k, Wi_v = Wi.chunk(3)\n",
    "Wi_q.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) $  \n",
    "now single head, so $i = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_ = Q.permute(1, 0, 2) # Batch first (B, L, D)\n",
    "K_ = K.permute(1, 0, 2)\n",
    "V_ = V.permute(1, 0, 2)\n",
    "QW = torch.matmul(Q_, Wi_q.T) # (B, L, D) dot (D, D)\n",
    "KW = torch.matmul(K_, Wi_k.T)\n",
    "VW = torch.matmul(V_, Wi_v.T)\n",
    "QW.shape # (B, L, D)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Attention(Q,K,V)=(Attention Weights)V \\\\\n",
    "Attention Weights = softmax({QK^T \\over \\sqrt{d_q}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 10])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "KW_t = KW.permute(0, 2, 1) # (B, D, L)\n",
    "QK_t_scaled = torch.bmm(QW, KW_t) / math.sqrt(D) # (B, L, D) batch dot (B, D, L) -> (B, L, L)\n",
    "attn_weights_ = F.softmax(QK_t_scaled, dim=-1)\n",
    "attn_weights_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.allclose(attn_weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 4])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = torch.bmm(attn_weights_, VW) # (B, L, L) (B, L, D) -> (B, L, D)\n",
    "attention.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ head_i = Attention(Q,K,V)\\\\MultiHead(Q,K,V)=Concat(head_1,â€¦,head_h)W^O $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 4])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_ = torch.matmul(attention, Wo.T) + bias\n",
    "attn_output_ = attn_output_.permute(1, 0, 2) # Batch second (L, B, D)\n",
    "attn_output_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.allclose(attn_output_, atol=1e-04)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a650650f6eb6c670a86107539d0d34d33ce8a7ed127d8748f3811dde1fb651a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
